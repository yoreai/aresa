import { NextRequest, NextResponse } from 'next/server'
import { exec } from 'child_process'
import { promisify } from 'util'

const execAsync = promisify(exec)

const ARESADB_PATH = process.env.ARESADB_PATH || '../../../tools/aresadb/target/release/aresadb'
const DB_PATH = process.env.ARESADB_DB_PATH || '/tmp/aresadb-studio-demo'

export async function POST(request: NextRequest) {
  try {
    const {
      query,
      table = 'medical_transcriptions',
      maxTokens = 4000,
    } = await request.json()

    if (!query || typeof query !== 'string') {
      return NextResponse.json(
        { error: 'Query is required' },
        { status: 400 }
      )
    }

    const startTime = performance.now()

    // Get relevant context via AresaDB RAG
    const contextCmd = `${ARESADB_PATH} --db ${DB_PATH} context "${query.replace(/"/g, '\\"')}" --table ${table} --max-tokens ${maxTokens} --format json`

    try {
      const { stdout: contextStdout } = await execAsync(contextCmd, {
        timeout: 30000,
        maxBuffer: 10 * 1024 * 1024,
      })

      let context
      try {
        context = JSON.parse(contextStdout)
      } catch {
        context = { chunks: [], totalTokens: 0 }
      }

      const endTime = performance.now()
      const retrievalTime = endTime - startTime

      // For now, return the retrieved context
      // In production, you would send this to an LLM for response generation
      return NextResponse.json({
        success: true,
        context,
        retrievalTime,
        query,
        // response would come from LLM integration
        // For demo, we return a placeholder
        response: generateDemoResponse(query, context),
      })
    } catch (execError: any) {
      return NextResponse.json({
        success: false,
        error: execError.stderr || execError.message,
        retrievalTime: performance.now() - startTime,
      }, { status: 400 })
    }
  } catch (error: any) {
    console.error('RAG API error:', error)
    return NextResponse.json(
      { error: error.message || 'Internal server error' },
      { status: 500 }
    )
  }
}

function generateDemoResponse(query: string, context: any): string {
  // Demo response generator
  // In production, this would call OpenAI/Anthropic/etc.

  const chunks = context.chunks || []
  const numSources = chunks.length

  return `Based on ${numSources} relevant documents from the medical knowledge base:

This is a demonstration response for the query: "${query}"

In a production deployment, this would be generated by an LLM (OpenAI, Anthropic, etc.)
using the ${context.totalTokens || 0} tokens of retrieved context.

Key sources retrieved:
${chunks.slice(0, 3).map((c: any, i: number) =>
  `${i + 1}. ${c.title || 'Document'} (similarity: ${(c.similarity * 100).toFixed(1)}%)`
).join('\n')}

The RAG pipeline successfully retrieved relevant context for your query.`
}

